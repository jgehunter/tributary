---
phase: 01-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/tributary/analytics/__init__.py
  - src/tributary/analytics/reader.py
  - tests/unit/test_reader.py
autonomous: true

must_haves:
  truths:
    - "User can query orderbook snapshots for any market and time range"
    - "User can query trade history for any market and time range"
    - "Reader leverages QuestDB SAMPLE BY for time-based aggregations"
    - "Reader mirrors existing QuestDBWriter connection pattern"
  artifacts:
    - path: "src/tributary/analytics/__init__.py"
      provides: "Analytics module exports"
      exports: ["QuestDBReader"]
    - path: "src/tributary/analytics/reader.py"
      provides: "QuestDB read interface for analytics"
      min_lines: 150
      exports: ["QuestDBReader"]
    - path: "tests/unit/test_reader.py"
      provides: "Unit tests for reader (mocked DB)"
      min_lines: 50
  key_links:
    - from: "src/tributary/analytics/reader.py"
      to: "QuestDBConfig"
      via: "import from core.config"
      pattern: "from tributary\\.core\\.config import QuestDBConfig"
    - from: "src/tributary/analytics/reader.py"
      to: "psycopg2"
      via: "database connection"
      pattern: "psycopg2\\.connect"
---

<objective>
Create QuestDBReader class that provides efficient read access to stored orderbook snapshots and trades.

Purpose: Analytics foundation - all cost calculations and benchmarks require querying historical data. The reader is the peer to the existing QuestDBWriter, completing the data access layer.

Output:
- `src/tributary/analytics/reader.py` - QuestDBReader class with query methods
- `tests/unit/test_reader.py` - Unit tests with mocked database
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-foundation/01-RESEARCH.md

# Existing patterns to follow
@src/tributary/storage/questdb.py
@src/tributary/core/config.py
@src/tributary/core/models.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create QuestDBReader class with connection management</name>
  <files>
    src/tributary/analytics/__init__.py
    src/tributary/analytics/reader.py
  </files>
  <action>
Create the analytics module and QuestDBReader class:

1. Create `src/tributary/analytics/__init__.py`:
   - Export QuestDBReader from reader module

2. Create `src/tributary/analytics/reader.py`:
   - Import QuestDBConfig from core.config
   - Use psycopg2 (already in dependencies) for PostgreSQL wire protocol
   - Mirror QuestDBWriter's connection pattern:
     - `__init__(self, config: QuestDBConfig)` - store config
     - `connect() -> None` - establish connection with autocommit=True (QuestDB requires this)
     - `close() -> None` - close connection
     - `is_connected` property - check connection state
   - Use port 8812 (pg_port from config) for queries
   - Return pandas DataFrames from all query methods

Key implementation details from research:
- Set `conn.autocommit = True` immediately after connecting (QuestDB doesn't support transactions)
- Use context manager for cursors: `with self._conn.cursor() as cur:`
- Convert cursor results to DataFrame: `pd.DataFrame(rows, columns=[desc[0] for desc in cur.description])`
  </action>
  <verify>
- File exists: `src/tributary/analytics/reader.py`
- File exists: `src/tributary/analytics/__init__.py`
- `from tributary.analytics import QuestDBReader` works in Python REPL
- Class has connect(), close(), is_connected methods
  </verify>
  <done>
QuestDBReader class exists with connection management that mirrors QuestDBWriter pattern.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add data query methods for trades and orderbooks</name>
  <files>src/tributary/analytics/reader.py</files>
  <action>
Add query methods to QuestDBReader:

1. `query_trades(market_id, start_time, end_time, token_id=None) -> pd.DataFrame`:
   - Query trades table with time range filter
   - Optional token_id filter for outcome-specific queries
   - Return columns: timestamp, market_id, token_id, trade_id, price, size, side, value
   - Order by timestamp ascending

2. `query_orderbook_snapshots(market_id, start_time, end_time, token_id=None) -> pd.DataFrame`:
   - Query orderbook_snapshots table with time range filter
   - Optional token_id filter
   - Return columns: timestamp, market_id, token_id, best_bid, best_ask, mid_price, spread, spread_bps, bid_prices, bid_sizes, ask_prices, ask_sizes
   - IMPORTANT: bid_prices/ask_prices are stored as JSON strings - parse with json.loads() after fetching
   - Order by timestamp ascending

3. `query_vwap_sampled(market_id, start_time, end_time, interval="1h") -> pd.DataFrame`:
   - Use QuestDB SAMPLE BY for efficient time-bucketed VWAP
   - SQL: `SELECT timestamp, sum(price * size) / sum(size) as vwap, sum(size) as volume, count() as trade_count FROM trades WHERE ... SAMPLE BY {interval} ALIGN TO CALENDAR`
   - IMPORTANT: interval must be string-interpolated, not parameterized (QuestDB limitation)
   - Validate interval format before interpolating (e.g., "1h", "15m", "1d")

4. `execute_query(query, params=None) -> pd.DataFrame`:
   - Generic query execution for custom queries
   - Used internally and available for advanced use cases

Use parameterized queries (%s placeholders) for all user-provided values except SAMPLE BY interval.
  </action>
  <verify>
- `ruff check src/tributary/analytics/reader.py` passes
- Class has all four query methods
- Methods have proper type hints (datetime for times, Optional[str] for token_id)
  </verify>
  <done>
QuestDBReader can query trades and orderbook snapshots with time range and market filters.
SAMPLE BY-based VWAP aggregation is available for efficient time-bucketed queries.
  </done>
</task>

<task type="auto">
  <name>Task 3: Add unit tests for reader</name>
  <files>tests/unit/test_reader.py</files>
  <action>
Create unit tests for QuestDBReader with mocked database:

1. Test connection management:
   - `test_connect_success` - mock psycopg2.connect, verify autocommit set
   - `test_is_connected_before_connect` - should be False
   - `test_is_connected_after_connect` - should be True
   - `test_close_clears_connection` - is_connected False after close

2. Test query_trades:
   - `test_query_trades_returns_dataframe` - mock cursor, verify DataFrame returned
   - `test_query_trades_with_token_id` - verify token_id added to WHERE clause
   - `test_query_trades_empty_result` - returns empty DataFrame, not error

3. Test query_orderbook_snapshots:
   - `test_query_orderbooks_returns_dataframe` - mock cursor, verify DataFrame returned
   - `test_query_orderbooks_parses_json_columns` - verify bid_prices/ask_prices are lists

4. Test query_vwap_sampled:
   - `test_vwap_sampled_uses_sample_by` - verify SAMPLE BY in generated SQL
   - `test_vwap_sampled_interval_interpolation` - verify interval is interpolated correctly

Use pytest fixtures from conftest.py pattern. Mock psycopg2.connect using unittest.mock.patch.
  </action>
  <verify>
- `pytest tests/unit/test_reader.py -v` passes
- All tests use mocked database (no real QuestDB needed)
- Tests cover connection, trades, orderbooks, and SAMPLE BY queries
  </verify>
  <done>
Unit tests pass, verifying QuestDBReader behavior without requiring real database.
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. Module structure:
   ```
   src/tributary/analytics/
   |-- __init__.py
   |-- reader.py
   ```

2. Import test:
   ```bash
   python -c "from tributary.analytics import QuestDBReader; print('Import OK')"
   ```

3. Unit tests:
   ```bash
   pytest tests/unit/test_reader.py -v
   ```

4. Lint check:
   ```bash
   ruff check src/tributary/analytics/
   ```
</verification>

<success_criteria>
- QuestDBReader class exists with connect/close/is_connected
- query_trades() returns DataFrame with trade data
- query_orderbook_snapshots() returns DataFrame with orderbook data
- query_vwap_sampled() uses SAMPLE BY for efficient aggregation
- Unit tests pass with mocked database
- Code follows existing project conventions (dataclass config, psycopg2 connection)
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-01-SUMMARY.md`
</output>
